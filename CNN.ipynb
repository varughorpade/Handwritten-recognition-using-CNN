{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.datasets import mnist #import the input dataset\n",
    "from tensorflow.keras.models import Sequential #import model to quickly set up a neural network\n",
    "from tensorflow.keras.layers import Dense,Dropout,Conv2D,MaxPooling2D,Flatten #some parameters to control bias and variance\n",
    "from tensorflow.keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading data set\n",
    "(mnist_train_images,mnist_train_labels),(mnist_test_images,mnist_test_labels)=mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if k.image_data_format()=='channels_first':\n",
    "    train_images=mnist_train_images.reshape(mnist_train_images.shape[0],1,28,28)\n",
    "    test_images=mnist_test_images.reshape(mnist_test_images.shape[0],1,28,28)\n",
    "    input_shape=(1,28,28)\n",
    "else:\n",
    "    train_images=mnist_train_images.reshape(mnist_train_images.shape[0],28,28,1)\n",
    "    test_images=mnist_test_images.reshape(mnist_test_images.shape[0],28,28,1)\n",
    "    input_shape=(28,28,1)\n",
    "    \n",
    "train_images=train_images.astype('float32')\n",
    "test_images=test_images.astype('float32')\n",
    "train_images/=255\n",
    "test_images/=255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting our train and test labels into one hot format\n",
    "train_labels=keras.utils.to_categorical(mnist_train_labels,10)\n",
    "test_labels=keras.utils.to_categorical(mnist_test_labels,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def display_sample(num):\n",
    "    print(train_labels[num])\n",
    "    label=train_labels[num].argmax(axis=0)\n",
    "    image=train_images[num].reshape([28,28])\n",
    "    plt.title('Sample: %d label %d' %(num,label))\n",
    "    plt.imshow(image,cmap=plt.get_cmap('gray_r'))\n",
    "    plt.show()\n",
    "    \n",
    "display_sample(1254)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "model.add(Conv2D(32,kernel_size=(3,3),activation='relu',input_shape=input_shape))\n",
    "\n",
    "#64 3x3 kernals\n",
    "model.add(Conv2D(64,(3,3),activation='relu'))\n",
    "\n",
    "#reduce by taking the max of each 2x2 block\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "#dropout to avoid overfitting\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "#flatten the results to one dimension for passing the results to another layer\n",
    "model.add(Flatten())\n",
    "\n",
    "#a hidden layer to learn with\n",
    "model.add(Dense(128,activation='relu'))\n",
    "\n",
    "#add another dropout\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "#final categorization from 0-9 with\n",
    "model.add(Dense(10,activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               1179776   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 1,199,882\n",
      "Trainable params: 1,199,882\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "WARNING:tensorflow:From C:\\Users\\visha\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/10\n",
      " - 309s - loss: 0.1821 - acc: 0.9452 - val_loss: 0.0430 - val_acc: 0.9859\n",
      "Epoch 2/10\n",
      " - 308s - loss: 0.0752 - acc: 0.9778 - val_loss: 0.0425 - val_acc: 0.9849\n",
      "Epoch 3/10\n",
      " - 309s - loss: 0.0576 - acc: 0.9828 - val_loss: 0.0314 - val_acc: 0.9894\n",
      "Epoch 4/10\n",
      " - 303s - loss: 0.0464 - acc: 0.9858 - val_loss: 0.0283 - val_acc: 0.9915\n",
      "Epoch 5/10\n",
      " - 303s - loss: 0.0403 - acc: 0.9879 - val_loss: 0.0273 - val_acc: 0.9919\n",
      "Epoch 6/10\n",
      " - 307s - loss: 0.0334 - acc: 0.9892 - val_loss: 0.0284 - val_acc: 0.9919\n",
      "Epoch 7/10\n",
      " - 305s - loss: 0.0293 - acc: 0.9905 - val_loss: 0.0308 - val_acc: 0.9905\n",
      "Epoch 8/10\n",
      " - 303s - loss: 0.0276 - acc: 0.9913 - val_loss: 0.0297 - val_acc: 0.9920\n",
      "Epoch 9/10\n",
      " - 303s - loss: 0.0251 - acc: 0.9919 - val_loss: 0.0290 - val_acc: 0.9922\n",
      "Epoch 10/10\n",
      " - 303s - loss: 0.0232 - acc: 0.9926 - val_loss: 0.0303 - val_acc: 0.9922\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(train_images,train_labels,batch_size=32,epochs=10,verbose=2,validation_data=(test_images,test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss 0.03025356593312763\n",
      "test accuracy 0.9922\n"
     ]
    }
   ],
   "source": [
    "score=model.evaluate(test_images,test_labels,verbose=0)\n",
    "print('test loss',score[0])\n",
    "print('test accuracy',score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected conv2d_4_input to have 4 dimensions, but got array with shape (1, 784)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-dbf62a399d1b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mtest_image\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_images\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m784\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mpredicted_cat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_image\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredicted_cat\u001b[0m \u001b[1;33m!=\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1094\u001b[0m       \u001b[1;31m# batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1095\u001b[0m       x, _, _ = self._standardize_user_data(\n\u001b[1;32m-> 1096\u001b[1;33m           x, check_steps=True, steps_name='steps', steps=steps)\n\u001b[0m\u001b[0;32m   1097\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1098\u001b[0m     if (self.run_eagerly or (isinstance(x, iterator_ops.EagerIterator) and\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle)\u001b[0m\n\u001b[0;32m   2380\u001b[0m         \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2381\u001b[0m         \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2382\u001b[1;33m         exception_prefix='input')\n\u001b[0m\u001b[0;32m   2383\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2384\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    351\u001b[0m                            \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m                            \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' dimensions, but got array '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 353\u001b[1;33m                            'with shape ' + str(data_shape))\n\u001b[0m\u001b[0;32m    354\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    355\u001b[0m           \u001b[0mdata_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected conv2d_4_input to have 4 dimensions, but got array with shape (1, 784)"
     ]
    }
   ],
   "source": [
    "for x in range(1000):\n",
    "    test_image=test_images[x,:].reshape(1,784)\n",
    "    predicted_cat=model.predict(test_image).argmax()\n",
    "    label=test_labels[x].argmax()\n",
    "    if(predicted_cat !=label):\n",
    "        plt.title('prediction %d label %d' %(predicted_cat,label))\n",
    "        plt.imshow(test_image.reshape([28,28]),cmap=plt.get_cmap('gray_r'))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
